/**
 * AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
 * Generated by scripts/generate-model-config.js from config/models.json
 * Last updated: 2025-10-18T16:19:33.804Z
 */

export type AIModel =
  | 'gpt-5-pro'
  | 'gpt-5'
  | 'gpt-5-mini'
  | 'gpt-5-nano'
  | 'o3'
  | 'o3-pro'
  | 'o3-mini'
  | 'o4-mini'
  | 'gpt-4.1'
  | 'gpt-4.1-mini'
  | 'gpt-4o'
  | 'gpt-4o-mini'
  | 'o1'
  | 'gemini-2.5-pro'
  | 'gemini-2.5-flash'
  | 'claude-sonnet-4-5-20250929'
  | 'claude-opus-4-1-20250805'
  | 'claude-opus-4-20250514'
  | 'claude-sonnet-4-20250514'
  | 'claude-3-7-sonnet-20250219'
  | 'claude-haiku-4-5'
  | 'claude-3-5-haiku-20241022'
  | 'grok-4-fast-reasoning';

export interface ModelConfigType {
  id: AIModel;
  provider: 'openai' | 'anthropic' | 'google' | 'xai' | 'ollama';
  client: string;
  displayName: string;
  description: string;
  year: number;
  contextWindow: number;
  contextWindowBeta?: number;
  maxOutput: number;
  status: 'available' | 'deprecated';
  recommended: boolean;
  order: number;
}

const MODELS: ModelConfigType[] = [
  {
    "id": "gpt-5-pro",
    "provider": "openai",
    "client": "OpenAIClient",
    "displayName": "GPT-5 Pro",
    "description": "Extended reasoning for most complex tasks",
    "year": 2025,
    "contextWindow": 400000,
    "maxOutput": 128000,
    "status": "available",
    "recommended": false,
    "order": 1
  },
  {
    "id": "gpt-5",
    "provider": "openai",
    "client": "OpenAIClient",
    "displayName": "GPT-5",
    "description": "Most advanced model for coding and agentic tasks",
    "year": 2025,
    "contextWindow": 400000,
    "maxOutput": 128000,
    "status": "available",
    "recommended": false,
    "order": 2
  },
  {
    "id": "gpt-5-mini",
    "provider": "openai",
    "client": "OpenAIClient",
    "displayName": "GPT-5 Mini",
    "description": "Faster, cost-efficient version",
    "year": 2025,
    "contextWindow": 200000,
    "maxOutput": 64000,
    "status": "available",
    "recommended": false,
    "order": 3
  },
  {
    "id": "gpt-5-nano",
    "provider": "openai",
    "client": "OpenAIClient",
    "displayName": "GPT-5 Nano",
    "description": "Fastest, most cost-efficient version",
    "year": 2025,
    "contextWindow": 128000,
    "maxOutput": 32000,
    "status": "available",
    "recommended": false,
    "order": 4
  },
  {
    "id": "o3",
    "provider": "openai",
    "client": "OpenAIClient",
    "displayName": "O3",
    "description": "Most powerful reasoning model",
    "year": 2025,
    "contextWindow": 200000,
    "maxOutput": 32768,
    "status": "available",
    "recommended": false,
    "order": 5
  },
  {
    "id": "o3-pro",
    "provider": "openai",
    "client": "OpenAIClient",
    "displayName": "O3 Pro",
    "description": "O3 with extended thinking",
    "year": 2025,
    "contextWindow": 200000,
    "maxOutput": 32768,
    "status": "available",
    "recommended": false,
    "order": 6
  },
  {
    "id": "o3-mini",
    "provider": "openai",
    "client": "OpenAIClient",
    "displayName": "O3 Mini",
    "description": "Cost-efficient reasoning model",
    "year": 2025,
    "contextWindow": 200000,
    "maxOutput": 100000,
    "status": "available",
    "recommended": false,
    "order": 7
  },
  {
    "id": "o4-mini",
    "provider": "openai",
    "client": "OpenAIClient",
    "displayName": "O4 Mini",
    "description": "Fast, cost-efficient reasoning model",
    "year": 2025,
    "contextWindow": 200000,
    "maxOutput": 65536,
    "status": "available",
    "recommended": false,
    "order": 8
  },
  {
    "id": "gpt-4.1",
    "provider": "openai",
    "client": "OpenAIClient",
    "displayName": "GPT-4.1",
    "description": "Smartest non-reasoning model, 1M context",
    "year": 2025,
    "contextWindow": 1000000,
    "maxOutput": 32768,
    "status": "available",
    "recommended": false,
    "order": 9
  },
  {
    "id": "gpt-4.1-mini",
    "provider": "openai",
    "client": "OpenAIClient",
    "displayName": "GPT-4.1 Mini",
    "description": "Fast, capable, efficient small model",
    "year": 2025,
    "contextWindow": 128000,
    "maxOutput": 16384,
    "status": "available",
    "recommended": false,
    "order": 10
  },
  {
    "id": "gpt-4o",
    "provider": "openai",
    "client": "OpenAIClient",
    "displayName": "GPT-4o",
    "description": "Fast, intelligent, flexible GPT model",
    "year": 2024,
    "contextWindow": 128000,
    "maxOutput": 16384,
    "status": "available",
    "recommended": false,
    "order": 11
  },
  {
    "id": "gpt-4o-mini",
    "provider": "openai",
    "client": "OpenAIClient",
    "displayName": "GPT-4o Mini",
    "description": "Fast, affordable small model",
    "year": 2024,
    "contextWindow": 128000,
    "maxOutput": 16384,
    "status": "available",
    "recommended": false,
    "order": 12
  },
  {
    "id": "o1",
    "provider": "openai",
    "client": "OpenAIClient",
    "displayName": "O1",
    "description": "Previous full o-series reasoning model",
    "year": 2024,
    "contextWindow": 200000,
    "maxOutput": 32768,
    "status": "available",
    "recommended": false,
    "order": 13
  },
  {
    "id": "gemini-2.5-pro",
    "provider": "google",
    "client": "GeminiClient",
    "displayName": "Gemini 2.5 Pro",
    "description": "State-of-the-art thinking model",
    "year": 2025,
    "contextWindow": 1048576,
    "maxOutput": 65536,
    "status": "available",
    "recommended": false,
    "order": 14
  },
  {
    "id": "gemini-2.5-flash",
    "provider": "google",
    "client": "GeminiClient",
    "displayName": "Gemini 2.5 Flash",
    "description": "Best price-performance with thinking",
    "year": 2025,
    "contextWindow": 1048576,
    "maxOutput": 65536,
    "status": "available",
    "recommended": false,
    "order": 15
  },
  {
    "id": "claude-sonnet-4-5-20250929",
    "provider": "anthropic",
    "client": "ClaudeClient",
    "displayName": "Claude 4.5 Sonnet",
    "description": "RECOMMENDED for complex agents",
    "year": 2025,
    "contextWindow": 200000,
    "contextWindowBeta": 1000000,
    "maxOutput": 64000,
    "status": "available",
    "recommended": true,
    "order": 16
  },
  {
    "id": "claude-opus-4-1-20250805",
    "provider": "anthropic",
    "client": "ClaudeClient",
    "displayName": "Claude Opus 4.1",
    "description": "Exceptional for specialized tasks",
    "year": 2025,
    "contextWindow": 200000,
    "maxOutput": 64000,
    "status": "available",
    "recommended": false,
    "order": 17
  },
  {
    "id": "claude-opus-4-20250514",
    "provider": "anthropic",
    "client": "ClaudeClient",
    "displayName": "Claude Opus 4",
    "description": "Previous flagship model",
    "year": 2025,
    "contextWindow": 200000,
    "contextWindowBeta": 1000000,
    "maxOutput": 64000,
    "status": "available",
    "recommended": false,
    "order": 18
  },
  {
    "id": "claude-sonnet-4-20250514",
    "provider": "anthropic",
    "client": "ClaudeClient",
    "displayName": "Claude Sonnet 4",
    "description": "High-performance model",
    "year": 2025,
    "contextWindow": 200000,
    "contextWindowBeta": 1000000,
    "maxOutput": 64000,
    "status": "available",
    "recommended": false,
    "order": 19
  },
  {
    "id": "claude-3-7-sonnet-20250219",
    "provider": "anthropic",
    "client": "ClaudeClient",
    "displayName": "Claude 3.7 Sonnet",
    "description": "With toggleable extended thinking",
    "year": 2025,
    "contextWindow": 200000,
    "maxOutput": 128000,
    "status": "available",
    "recommended": false,
    "order": 20
  },
  {
    "id": "claude-haiku-4-5",
    "provider": "anthropic",
    "client": "ClaudeClient",
    "displayName": "Claude 4.5 Haiku",
    "description": "Fastest model with near-frontier intelligence",
    "year": 2025,
    "contextWindow": 200000,
    "maxOutput": 64000,
    "status": "available",
    "recommended": false,
    "order": 21
  },
  {
    "id": "claude-3-5-haiku-20241022",
    "provider": "anthropic",
    "client": "ClaudeClient",
    "displayName": "Claude 3.5 Haiku",
    "description": "Fastest model (Older version)",
    "year": 2024,
    "contextWindow": 200000,
    "maxOutput": 8192,
    "status": "available",
    "recommended": false,
    "order": 22
  },
  {
    "id": "grok-4-fast-reasoning",
    "provider": "xai",
    "client": "GrokClient",
    "displayName": "Grok 4 Fast Reasoning",
    "description": "Fast reasoning with 2M context",
    "year": 2025,
    "contextWindow": 2000000,
    "maxOutput": 65536,
    "status": "available",
    "recommended": false,
    "order": 23
  }
];

const PROVIDER_INFO = {
  "openai": {
    "name": "OpenAI",
    "apiKeyEnvVar": "OPENAI_API_KEY"
  },
  "anthropic": {
    "name": "Anthropic",
    "apiKeyEnvVar": "ANTHROPIC_API_KEY"
  },
  "google": {
    "name": "Google",
    "apiKeyEnvVar": "GOOGLE_API_KEY"
  },
  "xai": {
    "name": "xAI",
    "apiKeyEnvVar": "XAI_API_KEY"
  },
  "ollama": {
    "name": "Ollama",
    "apiKeyEnvVar": null
  }
};

/**
 * Model configuration manager for VS Code extension
 */
export class ModelConfig {
  /**
   * Get configuration for a specific model
   */
  static get(modelId: string) {
    return MODELS.find(m => m.id === modelId);
  }

  /**
   * Get all active (non-deprecated) models
   */
  static getAllActive() {
    return MODELS.filter(m => m.status === 'available').sort((a, b) => a.order - b.order);
  }

  /**
   * Get all models (including deprecated)
   */
  static getAll() {
    return [...MODELS];
  }

  /**
   * Get models by provider
   */
  static getByProvider(provider: string) {
    return MODELS.filter(m => m.provider === provider);
  }

  /**
   * Get provider information
   */
  static getProviderInfo(provider: string) {
    return PROVIDER_INFO[provider];
  }

  /**
   * Get recommended models
   */
  static getRecommended() {
    return MODELS.filter(m => m.recommended && m.status === 'available');
  }

  /**
   * Get client class name for a model
   */
  static getClient(modelId: string) {
    const model = this.get(modelId);
    return model?.client;
  }

  /**
   * Get display name for a model
   */
  static getDisplayName(modelId: string) {
    const model = this.get(modelId);
    return model?.displayName || modelId;
  }

  /**
   * Get provider name for a model
   */
  static getProvider(modelId: string) {
    const model = this.get(modelId);
    if (!model) return undefined;
    return PROVIDER_INFO[model.provider]?.name;
  }
}
